# 每日从arXiv中获取最新YOLO相关论文


## YOLO\-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection

**发布日期**：2024-06-17

**作者**：Tamara R. Lenhard

**摘要**：Predominant methods for image\-based drone detection frequently rely on
employing generic object detection algorithms like YOLOv5. While proficient in
identifying drones against homogeneous backgrounds, these algorithms often
struggle in complex, highly textured environments. In such scenarios, drones
seamlessly integrate into the background, creating camouflage effects that
adversely affect the detection quality. To address this issue, we introduce a
novel deep learning architecture called YOLO\-FEDER FusionNet. Unlike
conventional approaches, YOLO\-FEDER FusionNet combines generic object detection
methods with the specialized strength of camouflage object detection techniques
to enhance drone detection capabilities. Comprehensive evaluations of
YOLO\-FEDER FusionNet show the efficiency of the proposed model and demonstrate
substantial improvements in both reducing missed detections and false alarms.


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.11641v1)

---


## ChildDiffusion: Unlocking the Potential of Generative AI and Controllable Augmentations for Child Facial Data using Stable Diffusion and Large Language Models

**发布日期**：2024-06-17

**作者**：Muhammad Ali Farooq

**摘要**：In this research work we have proposed high\-level ChildDiffusion framework
capable of generating photorealistic child facial samples and further embedding
several intelligent augmentations on child facial data using short text
prompts, detailed textual guidance from LLMs, and further image to image
transformation using text guidance control conditioning thus providing an
opportunity to curate fully synthetic large scale child datasets. The framework
is validated by rendering high\-quality child faces representing ethnicity data,
micro expressions, face pose variations, eye blinking effects, facial
accessories, different hair colours and styles, aging, multiple and different
child gender subjects in a single frame. Addressing privacy concerns regarding
child data acquisition requires a comprehensive approach that involves legal,
ethical, and technological considerations. Keeping this in view this framework
can be adapted to synthesise child facial data which can be effectively used
for numerous downstream machine learning tasks. The proposed method circumvents
common issues encountered in generative AI tools, such as temporal
inconsistency and limited control over the rendered outputs. As an exemplary
use case we have open\-sourced child ethnicity data consisting of 2.5k child
facial samples of five different classes which includes African, Asian, White,
South Asian/ Indian, and Hispanic races by deploying the model in production
inference phase. The rendered data undergoes rigorous qualitative as well as
quantitative tests to cross validate its efficacy and further fine\-tuning Yolo
architecture for detecting and classifying child ethnicity as an exemplary
downstream machine learning task.


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.11592v1)

---


## YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain

**发布日期**：2024-06-14

**作者**：Mujadded Al Rabbani Alif

**摘要**：This survey investigates the transformative potential of various YOLO
variants, from YOLOv1 to the state\-of\-the\-art YOLOv10, in the context of
agricultural advancements. The primary objective is to elucidate how these
cutting\-edge object detection models can re\-energise and optimize diverse
aspects of agriculture, ranging from crop monitoring to livestock management.
It aims to achieve key objectives, including the identification of contemporary
challenges in agriculture, a detailed assessment of YOLO's incremental
advancements, and an exploration of its specific applications in agriculture.
This is one of the first surveys to include the latest YOLOv10, offering a
fresh perspective on its implications for precision farming and sustainable
agricultural practices in the era of Artificial Intelligence and automation.
Further, the survey undertakes a critical analysis of YOLO's performance,
synthesizes existing research, and projects future trends. By scrutinizing the
unique capabilities packed in YOLO variants and their real\-world applications,
this survey provides valuable insights into the evolving relationship between
YOLO variants and agriculture. The findings contribute towards a nuanced
understanding of the potential for precision farming and sustainable
agricultural practices, marking a significant step forward in the integration
of advanced object detection technologies within the agricultural sector.


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.10139v1)

---


## A Deep Learning Approach to Detect Complete Safety Equipment For Construction Workers Based On YOLOv7

**发布日期**：2024-06-11

**作者**：Md. Shariful Islam

**摘要**：In the construction sector, ensuring worker safety is of the utmost
significance. In this study, a deep learning\-based technique is presented for
identifying safety gear worn by construction workers, such as helmets, goggles,
jackets, gloves, and footwears. The recommended approach uses the YOLO v7 \(You
Only Look Once\) object detection algorithm to precisely locate these safety
items. The dataset utilized in this work consists of labeled images split into
training, testing and validation sets. Each image has bounding box labels that
indicate where the safety equipment is located within the image. The model is
trained to identify and categorize the safety equipment based on the labeled
dataset through an iterative training approach. We used custom dataset to train
this model. Our trained model performed admirably well, with good precision,
recall, and F1\-score for safety equipment recognition. Also, the model's
evaluation produced encouraging results, with a mAP@0.5 score of 87.7\\%. The
model performs effectively, making it possible to quickly identify safety
equipment violations on building sites. A thorough evaluation of the outcomes
reveals the model's advantages and points up potential areas for development.
By offering an automatic and trustworthy method for safety equipment detection,
this research makes a contribution to the fields of computer vision and
workplace safety. The proposed deep learning\-based approach will increase
safety compliance and reduce the risk of accidents in the construction industry


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.07707v2)

---


## Advancing Roadway Sign Detection with YOLO Models and Transfer Learning

**发布日期**：2024-06-11

**作者**：Selvia Nafaa

**摘要**：Roadway signs detection and recognition is an essential element in the
Advanced Driving Assistant Systems \(ADAS\). Several artificial intelligence
methods have been used widely among of them YOLOv5 and YOLOv8. In this paper,
we used a modified YOLOv5 and YOLOv8 to detect and classify different roadway
signs under different illumination conditions. Experimental results indicated
that for the YOLOv8 model, varying the number of epochs and batch size yields
consistent MAP50 scores, ranging from 94.6% to 97.1% on the testing set. The
YOLOv5 model demonstrates competitive performance, with MAP50 scores ranging
from 92.4% to 96.9%. These results suggest that both models perform well across
different training setups, with YOLOv8 generally achieving slightly higher
MAP50 scores. These findings suggest that both models can perform well under
different training setups, offering valuable insights for practitioners seeking
reliable and adaptable solutions in object detection applications.


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.09437v1)

---


## Automated Pavement Cracks Detection and Classification Using Deep Learning

**发布日期**：2024-06-11

**作者**：Selvia Nafaa

**摘要**：Monitoring asset conditions is a crucial factor in building efficient
transportation asset management. Because of substantial advances in image
processing, traditional manual classification has been largely replaced by
semi\-automatic/automatic techniques. As a result, automated asset detection and
classification techniques are required. This paper proposes a methodology to
detect and classify roadway pavement cracks using the well\-known You Only Look
Once \(YOLO\) version five \(YOLOv5\) and version 8 \(YOLOv8\) algorithms.
Experimental results indicated that the precision of pavement crack detection
reaches up to 67.3% under different illumination conditions and image sizes.
The findings of this study can assist highway agencies in accurately detecting
and classifying asset conditions under different illumination conditions. This
will reduce the cost and time that are associated with manual inspection, which
can greatly reduce the cost of highway asset maintenance.


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.07674v1)

---


## Mamba YOLO: SSMs\-Based YOLO For Object Detection

**发布日期**：2024-06-09

**作者**：Zeyu Wang

**摘要**：Propelled by the rapid advancement of deep learning technologies, the YOLO
series has set a new benchmark for real\-time object detectors. Researchers have
continuously explored innovative applications of reparameterization, efficient
layer aggregation networks, and anchor\-free techniques on the foundation of
YOLO. To further enhance detection performance, Transformer\-based structures
have been introduced, significantly expanding the model's receptive field and
achieving notable performance gains. However, such improvements come at a cost,
as the quadratic complexity of the self\-attention mechanism increases the
computational burden of the model. Fortunately, the emergence of State Space
Models \(SSM\) as an innovative technology has effectively mitigated the issues
caused by quadratic complexity. In light of these advancements, we introduce
Mamba\-YOLO a novel object detection model based on SSM. Mamba\-YOLO not only
optimizes the SSM foundation but also adapts specifically for object detection
tasks. Given the potential limitations of SSM in sequence modeling, such as
insufficient receptive field and weak image locality, we have designed the
LSBlock and RGBlock. These modules enable more precise capture of local image
dependencies and significantly enhance the robustness of the model. Extensive
experimental results on the publicly available benchmark datasets COCO and VOC
demonstrate that Mamba\-YOLO surpasses the existing YOLO series models in both
performance and competitiveness, showcasing its substantial potential and
competitive edge.The PyTorch code is available
at:\\url\{https://github.com/HZAI\-ZJNU/Mamba\-YOLO\}


**代码链接**：https://github.com/HZAI-ZJNU/Mamba-YOLO}

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.05835v1)

---


## LW\-DETR: A Transformer Replacement to YOLO for Real\-Time Detection

**发布日期**：2024-06-05

**作者**：Qiang Chen

**摘要**：In this paper, we present a light\-weight detection transformer, LW\-DETR,
which outperforms YOLOs for real\-time object detection. The architecture is a
simple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our
approach leverages recent advanced techniques, such as training\-effective
techniques, e.g., improved loss and pretraining, and interleaved window and
global attentions for reducing the ViT encoder complexity. We improve the ViT
encoder by aggregating multi\-level feature maps, and the intermediate and final
feature maps in the ViT encoder, forming richer feature maps, and introduce
window\-major feature map organization for improving the efficiency of
interleaved attention computation. Experimental results demonstrate that the
proposed approach is superior over existing real\-time detectors, e.g., YOLO and
its variants, on COCO and other benchmark datasets. Code and models are
available at \(https://github.com/Atten4Vis/LW\-DETR\).


**代码链接**：https://github.com/Atten4Vis/LW-DETR).

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.03459v1)

---


## Open\-YOLO 3D: Towards Fast and Accurate Open\-Vocabulary 3D Instance Segmentation

**发布日期**：2024-06-04

**作者**：Mohamed El Amine Boudjoghra

**摘要**：Recent works on open\-vocabulary 3D instance segmentation show strong promise,
but at the cost of slow inference speed and high computation requirements. This
high computation cost is typically due to their heavy reliance on 3D clip
features, which require computationally expensive 2D foundation models like
Segment Anything \(SAM\) and CLIP for multi\-view aggregation into 3D. As a
consequence, this hampers their applicability in many real\-world applications
that require both fast and accurate predictions. To this end, we propose a fast
yet accurate open\-vocabulary 3D instance segmentation approach, named Open\-YOLO
3D, that effectively leverages only 2D object detection from multi\-view RGB
images for open\-vocabulary 3D instance segmentation. We address this task by
generating class\-agnostic 3D masks for objects in the scene and associating
them with text prompts. We observe that the projection of class\-agnostic 3D
point cloud instances already holds instance information; thus, using SAM might
only result in redundancy that unnecessarily increases the inference time. We
empirically find that a better performance of matching text prompts to 3D masks
can be achieved in a faster fashion with a 2D object detector. We validate our
Open\-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios:
\(i\) with ground truth masks, where labels are required for given object
proposals, and \(ii\) with class\-agnostic 3D proposals generated from a 3D
proposal network. Our Open\-YOLO 3D achieves state\-of\-the\-art performance on
both datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the
best existing method in literature. On ScanNet200 val. set, our Open\-YOLO 3D
achieves mean average precision \(mAP\) of 24.7\\% while operating at 22 seconds
per scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2406.02548v1)

---


## "Pass the butter": A study on desktop\-classic multitasking robotic arm based on advanced YOLOv7 and BERT

**发布日期**：2024-05-27

**作者**：Haohua Que

**摘要**：In recent years, various intelligent autonomous robots have begun to appear
in daily life and production. Desktop\-level robots are characterized by their
flexible deployment, rapid response, and suitability for light workload
environments. In order to meet the current societal demand for service robot
technology, this study proposes using a miniaturized desktop\-level robot \(by
ROS\) as a carrier, locally deploying a natural language model \(NLP\-BERT\), and
integrating visual recognition \(CV\-YOLO\) and speech recognition technology
\(ASR\-Whisper\) as inputs to achieve autonomous decision\-making and rational
action by the desktop robot. Three comprehensive experiments were designed to
validate the robotic arm, and the results demonstrate excellent performance
using this approach across all three experiments. In Task 1, the execution
rates for speech recognition and action performance were 92.6% and 84.3%,
respectively. In Task 2, the highest execution rates under the given conditions
reached 92.1% and 84.6%, while in Task 3, the highest execution rates were
95.2% and 80.8%, respectively. Therefore, it can be concluded that the proposed
solution integrating ASR, NLP, and other technologies on edge devices is
feasible and provides a technical and engineering foundation for realizing
multimodal desktop\-level robots.


**代码链接**：摘要中未找到代码链接。

**论文链接**：[阅读更多](http://arxiv.org/abs/2405.17250v1)

---

