# 每日从arXiv中获取最新YOLO相关论文


## Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE / 

发布日期：2025-08-25

作者：Abu Shad Ahammed

摘要：Ensuring safety in autonomous driving requires a seamless integration of perception and decision making under uncertain conditions. Although computer vision \(CV\) models such as YOLO achieve high accuracy in detecting traffic signs and obstacles, their performance degrades in drift scenarios caused by weather variations or unseen objects. This work presents a simulated autonomous driving system that combines a context aware CV model with adaptive control using the ADORE framework. The CARLA simulator was integrated with ADORE via the ROS bridge, allowing real\-time communication between perception, decision, and control modules. A simulated test case was designed in both clear and drift weather conditions to demonstrate the robust detection performance of the perception model while ADORE successfully adapted vehicle behavior to speed limits and obstacles with low response latency. The findings highlight the potential of coupling deep learning\-based perception with rule\-based adaptive decision making to improve automotive safety critical system.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2508.17985v1)

---


## Enhanced Drift\-Aware Computer Vision Architecture for Autonomous Driving / 

发布日期：2025-08-25

作者：Md Shahi Amran Hossain

摘要：The use of computer vision in automotive is a trending research in which safety and security are a primary concern. In particular, for autonomous driving, preventing road accidents requires highly accurate object detection under diverse conditions. To address this issue, recently the International Organization for Standardization \(ISO\) released the 8800 norm, providing structured frameworks for managing associated AI relevant risks. However, challenging scenarios such as adverse weather or low lighting often introduce data drift, leading to degraded model performance and potential safety violations. In this work, we present a novel hybrid computer vision architecture trained with thousands of synthetic image data from the road environment to improve robustness in unseen drifted environments. Our dual mode framework utilized YOLO version 8 for swift detection and incorporated a five\-layer CNN for verification. The system functioned in sequence and improved the detection accuracy by more than 90% when tested with drift\-augmented road images. The focus was to demonstrate how such a hybrid model can provide better road safety when working together in a hybrid structure.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2508.17975v1)

---


## A Synthetic Dataset for Manometry Recognition in Robotic Applications / 

发布日期：2025-08-24

作者：Pedro Antonio Rabelo Saraiva

摘要：This work addresses the challenges of data scarcity and high acquisition costs for training robust object detection models in complex industrial environments, such as offshore oil platforms. The practical and economic barriers to collecting real\-world data in these hazardous settings often hamper the development of autonomous inspection systems. To overcome this, in this work we propose and validate a hybrid data synthesis pipeline that combines procedural rendering with AI\-driven video generation. Our methodology leverages BlenderProc to create photorealistic images with precise annotations and controlled domain randomization, and integrates NVIDIA's Cosmos\-Predict2 world\-foundation model to synthesize physically plausible video sequences with temporal diversity, capturing rare viewpoints and adverse conditions. We demonstrate that a YOLO\-based detection network trained on a composite dataset, blending real images with our synthetic data, achieves superior performance compared to models trained exclusively on real\-world data. Notably, a 1:1 mixture of real and synthetic data yielded the highest accuracy, surpassing the real\-only baseline. These findings highlight the viability of a synthetic\-first approach as an efficient, cost\-effective, and safe alternative for developing reliable perception systems in safety\-critical and resource\-constrained industrial applications.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2508.17468v1)

---


## GW\-YOLO: Multi\-transient segmentation in LIGO using computer vision / 

发布日期：2025-08-24

作者：Siddharth Soni

摘要：Time series data and their time\-frequency representation from gravitational\-wave interferometers present multiple opportunities for the use of artificial intelligence methods associated with signal and image processing. Closely connected with this is the real\-time aspect associated with gravitational\-wave interferometers and the astrophysical observations they perform; the discovery potential of these instruments can be significantly enhanced when data processing can be achieved in O\(1s\) timescales. In this work, we introduce a novel signal and noise identification tool based on the YOLO \(You Only Look Once\) object detection framework. For its application into gravitational waves, we will refer to it as GW\-YOLO. This tool can provide scene identification capabilities and essential information regarding whether an observed transient is any combination of noise and signal. Additionally, it supplies detailed time\-frequency coordinates of the detected objects in the form of pixel masks, an essential property that can be used to understand and characterize astrophysical sources, as well as instrumental noise. The simultaneous identification of noise and signal, combined with precise pixel\-level localization, represents a significant advancement in gravitational\-wave data analysis. Our approach yields a 50% detection efficiency for binary black hole signals at a signal\-to\-noise ratio \(SNR\) of 15 when such signals overlap with transient noise artifacts. When noise artifacts overlap with binary neutron star signals, our algorithm attains 50% detection efficiency at an SNR of 30. This presents the first quantitative assessment of the ability to detect astrophysical events overlapping with realistic, instrument noise present in gravitational\-wave interferometers.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2508.17399v1)

---


## An Investigation of Visual Foundation Models Robustness / 

发布日期：2025-08-22

作者：Sandeep Gupta

摘要：Visual Foundation Models \(VFMs\) are becoming ubiquitous in computer vision, powering systems for diverse tasks such as object detection, image classification, segmentation, pose estimation, and motion tracking. VFMs are capitalizing on seminal innovations in deep learning models, such as LeNet\-5, AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver superior performance across a range of critical computer vision applications. These include security\-sensitive domains like biometric verification, autonomous vehicle perception, and medical image analysis, where robustness is essential to fostering trust between technology and the end\-users. This article investigates network robustness requirements crucial in computer vision systems to adapt effectively to dynamic environments influenced by factors such as lighting, weather conditions, and sensor characteristics. We examine the prevalent empirical defenses and robust training employed to enhance vision network robustness against real\-world challenges such as distributional shifts, noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we provide a comprehensive analysis of the challenges associated with these defense mechanisms, including network properties and components to guide ablation studies and benchmarking metrics to evaluate network robustness.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2508.16225v1)

---

