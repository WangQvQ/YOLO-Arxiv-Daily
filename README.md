# 每日从arXiv中获取最新YOLO相关论文


## Deep Learning\-Based Semantic Segmentation for Real\-Time Kidney Imaging and Measurements with Augmented Reality\-Assisted Ultrasound / 

发布日期：2025-06-30

作者：Gijs Luijten

摘要：Ultrasound \(US\) is widely accessible and radiation\-free but has a steep learning curve due to its dynamic nature and non\-standard imaging planes. Additionally, the constant need to shift focus between the US screen and the patient poses a challenge. To address these issues, we integrate deep learning \(DL\)\-based semantic segmentation for real\-time \(RT\) automated kidney volumetric measurements, which are essential for clinical assessment but are traditionally time\-consuming and prone to fatigue. This automation allows clinicians to concentrate on image interpretation rather than manual measurements. Complementing DL, augmented reality \(AR\) enhances the usability of US by projecting the display directly into the clinician's field of view, improving ergonomics and reducing the cognitive load associated with screen\-to\-patient transitions. Two AR\-DL\-assisted US pipelines on HoloLens\-2 are proposed: one streams directly via the application programming interface for a wireless setup, while the other supports any US device with video output for broader accessibility. We evaluate RT feasibility and accuracy using the Open Kidney Dataset and open\-source segmentation models \(nnU\-Net, Segmenter, YOLO with MedSAM and LiteMedSAM\). Our open\-source GitHub pipeline includes model implementations, measurement algorithms, and a Wi\-Fi\-based streaming solution, enhancing US training and diagnostics, especially in point\-of\-care settings.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2506.23721v1)

---


## DGE\-YOLO: Dual\-Branch Gathering and Attention for Accurate UAV Object Detection / 

发布日期：2025-06-29

作者：Kunwei Lv

摘要：The rapid proliferation of unmanned aerial vehicles \(UAVs\) has highlighted the importance of robust and efficient object detection in diverse aerial scenarios. Detecting small objects under complex conditions, however, remains a significant challenge. Existing approaches often prioritize inference speed, leading to degraded performance when handling multi\-modal inputs. To address this, we present DGE\-YOLO, an enhanced YOLO\-based detection framework designed to effectively fuse multi\-modal information. Specifically, we introduce a dual\-branch architecture for modality\-specific feature extraction, enabling the model to process both infrared and visible images. To further enrich semantic representation, we propose an Efficient Multi\-scale Attention \(EMA\) mechanism that enhances feature learning across spatial scales. Additionally, we replace the conventional neck with a Gather\-and\-Distribute module to mitigate information loss during feature aggregation. Extensive experiments on the Drone Vehicle dataset demonstrate that DGE\-YOLO achieves superior performance over state\-of\-the\-art methods, validating its effectiveness in multi\-modal UAV object detection tasks.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2506.23252v1)

---


## YM\-WML: A new Yolo\-based segmentation Model with Weighted Multi\-class Loss for medical imaging / 

发布日期：2025-06-28

作者：Haniyeh Nikkhah

摘要：Medical image segmentation poses significant challenges due to class imbalance and the complex structure of medical images. To address these challenges, this study proposes YM\-WML, a novel model for cardiac image segmentation. The model integrates a robust backbone for effective feature extraction, a YOLOv11 neck for multi\-scale feature aggregation, and an attention\-based segmentation head for precise and accurate segmentation. To address class imbalance, we introduce the Weighted Multi\-class Exponential \(WME\) loss function. On the ACDC dataset, YM\-WML achieves a Dice Similarity Coefficient of 91.02, outperforming state\-of\-the\-art methods. The model demonstrates stable training, accurate segmentation, and strong generalization, setting a new benchmark in cardiac segmentation tasks.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2506.22955v1)

---


## CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability / 

发布日期：2025-06-27

作者：Justin Reinman

摘要：CERBERUS is a synthetic benchmark designed to help train and evaluate AI models for detecting cracks and other defects in infrastructure. It includes a crack image generator and realistic 3D inspection scenarios built in Unity. The benchmark features two types of setups: a simple Fly\-By wall inspection and a more complex Underpass scene with lighting and geometry challenges. We tested a popular object detection model \(YOLO\) using different combinations of synthetic and real crack data. Results show that combining synthetic and real data improves performance on real\-world images. CERBERUS provides a flexible, repeatable way to test defect detection systems and supports future research in automated infrastructure inspection. CERBERUS is publicly available at https://github.com/justinreinman/Cerberus\-Defect\-Generator.

中文摘要：


代码链接：https://github.com/justinreinman/Cerberus-Defect-Generator.

论文链接：[阅读更多](http://arxiv.org/abs/2506.21909v1)

---


## Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment / 

发布日期：2025-06-27

作者：Dipayan Biswas

摘要：Video is transforming education with online courses and recorded lectures supplementing and replacing classroom teaching. Recent research has focused on enhancing information retrieval for video lectures with advanced navigation, searchability, summarization, as well as question answering chatbots. Visual elements like tables, charts, and illustrations are central to comprehension, retention, and data presentation in lecture videos, yet their full potential for improving access to video content remains underutilized. A major factor is that accurate automatic detection of visual elements in a lecture video is challenging; reasons include i\) most visual elements, such as charts, graphs, tables, and illustrations, are artificially created and lack any standard structure, and ii\) coherent visual objects may lack clear boundaries and may be composed of connected text and visual components. Despite advancements in deep learning based object detection, current models do not yield satisfactory performance due to the unique nature of visual content in lectures and scarcity of annotated datasets. This paper reports on a transfer learning approach for detecting visual elements in lecture video frames. A suite of state of the art object detection models were evaluated for their performance on lecture video datasets. YOLO emerged as the most promising model for this task. Subsequently YOLO was optimized for lecture video object detection with training on multiple benchmark datasets and deploying a semi\-supervised auto labeling strategy. Results evaluate the success of this approach, also in developing a general solution to the problem of object detection in lecture videos. Paper contributions include a publicly released benchmark of annotated lecture video frames, along with the source code to facilitate future research.

中文摘要：


代码链接：摘要中未找到代码链接。

论文链接：[阅读更多](http://arxiv.org/abs/2506.21903v1)

---

